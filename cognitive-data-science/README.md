#*“For Italians only”: exploring discriminative behavior in LLMs in a real-estate context*

## Project summary

This repository collects the materials, code and results for an experimental study that probes how large language models (LLMs) reproduce and amplify discriminatory signals in a realistic housing-rental scenario.
The project was developed as an experiment within the academic context of the **Cognitive Data Science** exam, involving the design of a full project pipeline and the production of a final scientific report.

The study applies techniques from **semantic network analysis** (including FormaMentis Networks and emotion-based semantic modeling) and positions itself within the emerging field of **Machine Psychology**, which investigates cognitive-like patterns emerging from artificial systems.
At the same time, the work contributes to discussions in **AI Ethics** and **Computational Social Science**, focusing on how algorithmic systems may replicate or amplify discriminatory biases relevant to real social contexts.

---

## Key points

* **Research question:** Do LLM outputs exhibit discriminatory behavior tied to names, occupations, age, neighborhood, or owner political orientation?
**Motivation:**
The motivation for this study arises from a broader set of considerations spanning social, psychological, and technological domains. First, within the Italian social context, the housing market itself raises important questions: *can discriminatory patterns emerge in rental dynamics, and how do these relate to well-established sociological findings on discrimination?* In this perspective, a person’s **name** acts as a powerful social cue, often implicitly conveying pre-judicial information about **gender, ethnicity, or religious background**, consistent with core theories in **social psychology**.
A second layer of motivation concerns the growing integration of **AI systems into real-world markets**. As these models increasingly mediate access to resources, it becomes crucial to ensure that they do not reproduce or amplify harmful biases.
Finally, this study explores whether LLMs can be used as **simulation tools** to model social interactions and test hypotheses in **sociological and psychological research**, offering a controlled environment for investigating complex social phenomena.
* **Outcome:** Strong signals of ethnic and class discrimination were found in LLM-generated owner replies. Political ideology and tenant name significantly influenced discriminatory outcomes. Emotional and semantic analyses revealed nuanced patterns such as positivity bias in owners and more complex negative emotions in tenants.

---

## Techniques & methods

### 1. Experimental design & data generation

* Structured a variable matrix defining owners’ and tenants’ demographic and contextual attributes.
* Generated **5,184 unique prompt combinations**, expanded to **20,736 interaction samples** including gender variants.
* Used **mistral-small** to generate owner and tenant texts simulating rental inquiry exchanges.

### 2. Automated labelling & emotion detection

* Used **GPT-4o-mini** for structured discrimination labeling across five categories (urban, ethnic, gender, age-based, class-based).
* Applied **EmoAtlas** to compute emotion profiles aligned with Plutchik’s model.
* Used **z-scores** to test emotional significance, adopting |z| ≥ 1.64 for exploratory detection due to LLM positivity bias.

### 3. Statistical testing

* Ran **chi-squared tests** to assess relationships between demographic attributes and discriminatory outcomes (e.g., name ↔ ethnicity, ideology ↔ ethnicity, occupation ↔ class, age ↔ ageism).

### 4. Semantic/network analysis

* Built **FormaMentis Networks (FMNs)** to examine semantic relationships in generated texts.
* Identified implicit discriminatory associations via shortest-path analyses and network centrality measures, even where no explicit negative content was present.

---

## Main findings

* **Ethnic discrimination** strongly correlated with tenant names.
* **Political ideology** of owners significantly influenced discriminatory patterns.
* **Class discrimination** correlated with neighborhood and occupation.
* **Ageism** correlated with tenant age.
* **Emotional patterns:** Owner replies showed consistent positivity bias; tenant messages contained richer negative-emotion structures.

---

## Limitations

* Interaction data was fully synthetic and generated by LLMs, limiting direct comparability to human behavior.
* Some variable (as names from different ethnicities) lists were generated by LLMs, reflecting model-driven assumptions.
* Positivity bias required adapted emotional thresholds.
* Discrimination labels were generated automatically and not human-validated.

---

## Conclusions

The study shows that LLMs, when placed into realistic housing-rental scenarios, can reproduce and amplify discriminatory associations linked to ethnicity, class, political cues and age.
Semantic network analysis and emotion modeling reveal both explicit and implicit pathways through which these patterns emerge.
The results highlight the importance of auditing, interpreting and mitigating biased model behavior, especially in domains where algorithmic decisions may impact social equity, access to resources, and perceptions of fairness.
